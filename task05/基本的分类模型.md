# 基本的分类模型

​		分类是一种基于一个或多个自变量确定因变量所属类别的技术，通常接触的有逻辑回归、KNN、SVM、朴素贝叶斯等。

## 一、**分类器**

**1）逻辑回归**

​		逻辑回归类似于线性回归，适用于因变量不是一个数值字的情况 (例如，一个“是/否”的响应)。它虽然被称为回归，但却是基于根据回归的分类，将因变量分为两类。函数给出了事件发生和不发生概率的对数。最后，根据这两类中较高的概率对变量进行分类。

**2）KNN**

​		K-NN算法是一种最简单的分类算法，通过识别被分成若干类的数据点，以预测新样本点的分类。K-NN是一种非参数的算法，是“懒惰学习”的著名代表，它根据相似性（如，距离函数）对新数据进行分类。其中k值设定大小的有以下trick：

​						k值变小：low bias，high variance

​						k值变大：high bias， low variance

**3）SVM**

​		支持向量机既可用于回归也可用于分类。它基于定义决策边界的决策平面。决策平面（超平面）可将一组属于不同类的对象分离开。通俗一些的语言就是找到一个能够通过最大间隔来分割不同类别的样本的线性超平面。

**4）朴素贝叶斯**

​		朴素贝叶斯分类器建立在贝叶斯定理的基础上，基于特征之间互相独立的假设（假定类中存在一个与任何其他特征无关的特征）。即使这些特征相互依赖，或者依赖于其他特征的存在，朴素贝叶斯算法都认为这些特征都是独立的。这样的假设过于理想，朴素贝叶斯因此而得名。

**5）决策树分类**

​		**决策树以树状结构构建分类或回归模型。**它通过将数据集不断拆分为更小的子集来使决策树不断生长。最终长成具有决策节点（包括根节点和内部节点）和叶节点的树。基础的算法有ID3、C4.5、CART。他们分别是根据信息增益，信息增益率，基尼系数进行分类的。

## **二、分类性能评估**

		- 真阳性TP：预测值和真实值都为正例；
		- 真阴性TN：预测值与真实值都为正例；
		- 假阳性FP：预测值为正，实际值为负；
		- 假阴性FN：预测值为负，实际值为正；



​	我们可以知道一些更具体的指标：

​		准确率：分类正确的样本数占总样本的比例，即：$$ACC = \frac{TP+TN}{FP+FN+TP+TN}$$

​		精度：预测为正且分类正确的样本占预测值为正的比例，即：$$PRE = \frac{TP}{TP+FP}$$

​		召回率：预测为正且分类正确的样本占类别为正的比例，即：$$REC =  \frac{TP}{TP+FN}$$

​		F1值：综合衡量精度和召回率，即：$$F1 = 2\frac{PRE\times REC}{PRE + REC}$$	

​		ROC曲线：以假阳率为横轴，真阳率为纵轴画出来的曲线，曲线下方面积越大越好

根据我们的任务不同，需要选择不一样的指标去衡量我们的分类模型，因为分类很多时候会因为训练样本分布的不一致，影响性能。也可能因为所聚焦的是检测异常值之类的。所以在衡量分类模型的时候，我们一般使用的指标都是多个指标的融合表现，比如F1值，ROC曲线。找到一个平衡点来评价我们整个模型的表现。

​	